# Step 1: Install required packages
!pip install faiss-cpu transformers sentence-transformers --quiet

# Step 2: Import libraries
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import faiss
import numpy as np

# Step 3: Load embedding and QA models
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Compact and fast
qa_pipeline = pipeline('text-generation', model='gpt2')    # Simple generation model

# Step 4: Create a simple knowledge base
documents = [
    "Python is a popular programming language created by Guido van Rossum.",
    "Google Colab is a free Jupyter notebook environment that runs in the cloud.",
    "The capital of France is Paris.",
    "The Great Wall of China was built to protect against invasions.",
    "Machine learning is a subset of artificial intelligence."
]

# Step 5: Generate embeddings for the documents
document_embeddings = embedding_model.encode(documents)

# Step 6: Build a FAISS index
index = faiss.IndexFlatL2(document_embeddings.shape[1])
index.add(np.array(document_embeddings))

# Step 7: Define a RAG function
def rag_query(query, top_k=2):
    query_embedding = embedding_model.encode([query])
    _, indices = index.search(query_embedding, top_k)
    
    # Get top-k relevant documents
    retrieved_docs = [documents[idx] for idx in indices[0]]
    context = " ".join(retrieved_docs)
    
    # Combine query and context
    prompt = f"Context: {context}\n\nQuestion: {query}\nAnswer:"
    
    # Generate answer using GPT2 (can be replaced with any better model)
    response = qa_pipeline(prompt, max_length=100, do_sample=True, temperature=0.7)
    print("Answer:", response[0]['generated_text'].split('Answer:')[-1].strip())

# Step 8: Test it
rag_query("Who invented Python?")
rag_query("What is the purpose of the Great Wall?")
